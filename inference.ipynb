{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02326af4",
   "metadata": {},
   "source": [
    "## 1. Load model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "510661e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device in use: mps\n",
      "Model loaded successfully!\n",
      "Model parameters: 470,466\n",
      "Model config: {'in_channels': 5, 'hidden_channels': 128, 'num_layers': 4, 'dropout': 0.15}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "# Define the model class (copy from your training script)\n",
    "class ParticleDynamicsGNN(MessagePassing):\n",
    "    \"\"\"Graph Neural Network for predicting particle dynamics in SPH simulations\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, hidden_channels=64, num_layers=3, dropout=0.1):\n",
    "        super().__init__(aggr='add')\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Input embedding\n",
    "        self.input_embedding = nn.Sequential(\n",
    "            nn.Linear(in_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Message passing layers\n",
    "        self.message_mlps = nn.ModuleList()\n",
    "        self.update_mlps = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(num_layers):\n",
    "            # Message MLP\n",
    "            message_mlp = nn.Sequential(\n",
    "                nn.Linear(2 * hidden_channels, hidden_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_channels, hidden_channels)\n",
    "            )\n",
    "            self.message_mlps.append(message_mlp)\n",
    "            \n",
    "            # Update MLP\n",
    "            update_mlp = nn.Sequential(\n",
    "                nn.Linear(2 * hidden_channels, hidden_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_channels, hidden_channels)\n",
    "            )\n",
    "            self.update_mlps.append(update_mlp)\n",
    "        \n",
    "        # Output layer for position prediction\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels // 2, 2)  # Predict 2D position\n",
    "        )\n",
    "        \n",
    "        # Skip connections\n",
    "        self.skip_connections = nn.ModuleList([\n",
    "            nn.Linear(hidden_channels, hidden_channels) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        # Input embedding\n",
    "        h = self.input_embedding(x)\n",
    "        \n",
    "        # Store for skip connections\n",
    "        residual = h\n",
    "        \n",
    "        # Message passing layers\n",
    "        for i in range(self.num_layers):\n",
    "            h_new = self.propagate(edge_index, x=h, layer_idx=i)\n",
    "            \n",
    "            # Skip connection\n",
    "            if i > 0:\n",
    "                h_new = h_new + self.skip_connections[i](residual)\n",
    "            \n",
    "            h = h_new\n",
    "            \n",
    "            # Update residual every 2 layers\n",
    "            if i % 2 == 1:\n",
    "                residual = h\n",
    "        \n",
    "        # Output layer\n",
    "        out = self.output_layer(h)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def message(self, x_i, x_j, layer_idx):\n",
    "        msg_input = torch.cat([x_i, x_j], dim=1)\n",
    "        msg = self.message_mlps[layer_idx](msg_input)\n",
    "        return msg\n",
    "    \n",
    "    def update(self, aggr_out, x, layer_idx):\n",
    "        update_input = torch.cat([x, aggr_out], dim=1)\n",
    "        updated = self.update_mlps[layer_idx](update_input)\n",
    "        return updated\n",
    "    \n",
    "    def propagate(self, edge_index, x, layer_idx):\n",
    "        row, col = edge_index\n",
    "        x_i = x[row]\n",
    "        x_j = x[col]\n",
    "        \n",
    "        msg = self.message(x_i, x_j, layer_idx)\n",
    "        \n",
    "        aggr_out = torch.zeros_like(x)\n",
    "        aggr_out.index_add_(0, row, msg)\n",
    "        \n",
    "        out = self.update(aggr_out, x, layer_idx)\n",
    "        return out\n",
    "\n",
    "# Load the trained model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "# device = torch.device('cpu');\n",
    "print(f\"Device in use: {device}\")\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load('./complete_gnn_physics_model_1.pth', map_location=device)\n",
    "\n",
    "# Get model config and create model instance\n",
    "model_config = checkpoint['model_config']\n",
    "model = ParticleDynamicsGNN(**model_config)\n",
    "\n",
    "# Load the state dict\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Model config: {model_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de5ea12",
   "metadata": {},
   "source": [
    "## 2. Measure inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93c69539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA PREPARATION PHASE\n",
      "============================================================\n",
      "Loading test data...\n",
      "Loading 2 episodes from test.h5\n",
      "  Loading episode 1/2...\n",
      "Final shape - Positions: torch.Size([2, 81, 5740, 2]), Types: torch.Size([2, 5740])\n",
      "Preparing test data...\n",
      "Created 152 test samples\n",
      "Test loader created with 76 batches\n",
      "Sample test data shape: torch.Size([5740, 5])\n",
      "\n",
      "============================================================\n",
      "DATA PREPARATION COMPLETE\n",
      "============================================================\n",
      "âœ“ Loaded 152 test samples\n",
      "âœ“ Created data loader with 76 batches\n",
      "âœ“ Sample graph: 5740 nodes, 56878 edges\n",
      "âœ“ Node features: 5 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Create test data from your training pipeline\n",
    "# First, let's load the same data preparation functions\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "def build_neighbor_graph(positions, radius=0.01, max_neighbors=10):\n",
    "    \"\"\"Build graph connectivity from particle positions\"\"\"\n",
    "    n_particles = positions.size(0)\n",
    "    \n",
    "    # Compute pairwise distances\n",
    "    pos_i = positions.unsqueeze(1)  # (N, 1, 2)\n",
    "    pos_j = positions.unsqueeze(0)  # (1, N, 2)\n",
    "    distances = torch.norm(pos_i - pos_j, dim=2)  # (N, N)\n",
    "    \n",
    "    # Find neighbors within radius (excluding self-connections)\n",
    "    mask = (distances <= radius) & (distances > 0)\n",
    "    \n",
    "    # Optional: Limit number of neighbors per particle\n",
    "    if max_neighbors is not None:\n",
    "        for i in range(n_particles):\n",
    "            neighbor_distances = distances[i]\n",
    "            neighbor_mask = mask[i]\n",
    "            \n",
    "            if neighbor_mask.sum() > max_neighbors:\n",
    "                # Keep only closest neighbors\n",
    "                neighbor_distances[~neighbor_mask] = float('inf')\n",
    "                _, closest_idx = neighbor_distances.topk(max_neighbors, largest=False)\n",
    "                \n",
    "                # Reset mask for this particle\n",
    "                mask[i] = False\n",
    "                mask[i, closest_idx] = True\n",
    "    \n",
    "    # Convert to edge_index format\n",
    "    edges = torch.nonzero(mask, as_tuple=False)\n",
    "    edge_index = edges.t().contiguous()\n",
    "    \n",
    "    return edge_index\n",
    "\n",
    "def create_graph_data(positions, particle_types, timestep_idx, target_positions=None):\n",
    "    \"\"\"Create PyTorch Geometric Data object from particle data\"\"\"\n",
    "    # Node features: [x, y, particle_type_one_hot, timestep_normalized]\n",
    "    num_types = particle_types.max().item() + 1\n",
    "    type_one_hot = F.one_hot(particle_types, num_classes=num_types).float()\n",
    "    \n",
    "    # Normalize timestep\n",
    "    timestep_feature = torch.full((len(positions), 1), timestep_idx / 100.0)\n",
    "    \n",
    "    # Combine features\n",
    "    x = torch.cat([positions, type_one_hot, timestep_feature], dim=1)\n",
    "    \n",
    "    # Build graph connectivity\n",
    "    edge_index = build_neighbor_graph(positions, radius=0.1)\n",
    "    \n",
    "    # Create data object\n",
    "    data = Data(x=x, edge_index=edge_index)\n",
    "    \n",
    "    if target_positions is not None:\n",
    "        data.y = target_positions\n",
    "    \n",
    "    return data\n",
    "\n",
    "def load_h5_dataset(filepath, max_episodes=None, skip_timesteps=1):\n",
    "    \"\"\"Load SPH dataset from H5 file with memory optimization\"\"\"\n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "        episodes = sorted(list(f.keys()))\n",
    "        if max_episodes:\n",
    "            episodes = episodes[:max_episodes]\n",
    "        \n",
    "        print(f\"Loading {len(episodes)} episodes from {os.path.basename(filepath)}\")\n",
    "        \n",
    "        all_positions = []\n",
    "        all_particle_types = []\n",
    "        \n",
    "        for i, episode_id in enumerate(episodes):\n",
    "            if i % 10 == 0:\n",
    "                print(f\"  Loading episode {i+1}/{len(episodes)}...\")\n",
    "                \n",
    "            # Load position data with optional timestep skipping\n",
    "            positions = f[f'{episode_id}/position'][::skip_timesteps]\n",
    "            particle_types = f[f'{episode_id}/particle_type'][:]\n",
    "            \n",
    "            all_positions.append(torch.tensor(positions, dtype=torch.float32))\n",
    "            all_particle_types.append(torch.tensor(particle_types, dtype=torch.long))\n",
    "            \n",
    "        # Stack all episodes\n",
    "        positions = torch.stack(all_positions)\n",
    "        particle_types = torch.stack(all_particle_types)\n",
    "        \n",
    "        print(f\"Final shape - Positions: {positions.shape}, Types: {particle_types.shape}\")\n",
    "        return positions, particle_types\n",
    "\n",
    "def prepare_training_data(positions, particle_types, sequence_length=5):\n",
    "    \"\"\"Prepare sequences of graph data for training\"\"\"\n",
    "    data_list = []\n",
    "    \n",
    "    for episode in range(positions.shape[0]):\n",
    "        episode_types = particle_types[episode]\n",
    "        \n",
    "        for t in range(positions.shape[1] - sequence_length):\n",
    "            # Current state\n",
    "            current_pos = positions[episode, t]\n",
    "            \n",
    "            # Target (next timestep)\n",
    "            target_pos = positions[episode, t + 1]\n",
    "            \n",
    "            # Create graph data\n",
    "            data = create_graph_data(current_pos, episode_types, t, target_pos)\n",
    "            data_list.append(data)\n",
    "    \n",
    "    return data_list\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATA PREPARATION PHASE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load test data (same as in your training pipeline)\n",
    "data_dir = '/Volumes/Meida/01-CodeLab/01-personal-project/GNN/2D_DAM_5740_20kevery100'\n",
    "test_path = os.path.join(data_dir, 'test.h5')\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "test_pos, test_types = load_h5_dataset(test_path, max_episodes=2, skip_timesteps=5)\n",
    "\n",
    "print(\"Preparing test data...\")\n",
    "test_data_list = prepare_training_data(test_pos, test_types)\n",
    "print(f\"Created {len(test_data_list)} test samples\")\n",
    "\n",
    "# Create test loader\n",
    "batch_size = 2  # Same as training\n",
    "test_loader = DataLoader(test_data_list, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Test loader created with {len(test_loader)} batches\")\n",
    "print(f\"Sample test data shape: {test_data_list[0].x.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA PREPARATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ“ Loaded {len(test_data_list)} test samples\")\n",
    "print(f\"âœ“ Created data loader with {len(test_loader)} batches\")\n",
    "print(f\"âœ“ Sample graph: {test_data_list[0].x.shape[0]} nodes, {test_data_list[0].edge_index.shape[1]} edges\")\n",
    "print(f\"âœ“ Node features: {test_data_list[0].x.shape[1]} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988c59d7",
   "metadata": {},
   "source": [
    "## 3. Collect data phase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f944679b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SINGLE TIMESTEP PERFORMANCE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "1. MEASURING TRUE SINGLE TIMESTEP PERFORMANCE\n",
      "--------------------------------------------------\n",
      "Single timestep info: 5740 nodes, 108451 edges\n",
      "Batch size: 1\n",
      "Running 10 warmup iterations...\n",
      "Running 100 timed iterations...\n",
      "  Completed 20/100 runs\n",
      "  Completed 40/100 runs\n",
      "  Completed 60/100 runs\n",
      "  Completed 80/100 runs\n",
      "  Completed 100/100 runs\n",
      "\n",
      "Single Timestep Statistics:\n",
      "  Mean: 225.482 ms\n",
      "  Std:  3.426 ms\n",
      "  Min:  218.577 ms\n",
      "  Max:  238.319 ms\n",
      "  P95:  230.781 ms\n",
      "  P99:  233.359 ms\n",
      "\n",
      "Single Timestep Throughput:\n",
      "  Timesteps/sec: 4.4\n",
      "  Time per node: 39.283 Î¼s\n",
      "  Time per edge: 2.079 Î¼s\n",
      "\n",
      "2. BATCH SIZE EFFICIENCY COMPARISON\n",
      "--------------------------------------------------\n",
      "============================================================\n",
      "BATCH VS SINGLE TIMESTEP COMPARISON\n",
      "============================================================\n",
      "\n",
      "--- Single Timestep (batch_size=1) ---\n",
      "Batch info: 5740 nodes, 108451 edges\n",
      "Running 5 warmup iterations...\n",
      "Running 50 timed iterations...\n",
      "  Completed 20/50 runs\n",
      "  Completed 40/50 runs\n",
      "  Total time: 226.00 ms\n",
      "  Time per timestep: 226.00 ms\n",
      "  Timesteps per second: 4.42\n",
      "  Nodes per timestep: 5740\n",
      "  Edges per timestep: 108451\n",
      "\n",
      "--- Batch of 2 (batch_size=2) ---\n",
      "Batch info: 11480 nodes, 216914 edges\n",
      "Running 5 warmup iterations...\n",
      "Running 50 timed iterations...\n",
      "  Completed 20/50 runs\n",
      "  Completed 40/50 runs\n",
      "  Total time: 434.97 ms\n",
      "  Time per timestep: 217.49 ms\n",
      "  Timesteps per second: 4.60\n",
      "  Nodes per timestep: 5740\n",
      "  Edges per timestep: 108457\n",
      "\n",
      "--- Batch of 4 (batch_size=4) ---\n",
      "Batch info: 22960 nodes, 433983 edges\n",
      "Running 5 warmup iterations...\n",
      "Running 50 timed iterations...\n",
      "  Completed 20/50 runs\n",
      "  Completed 40/50 runs\n",
      "  Total time: 863.95 ms\n",
      "  Time per timestep: 215.99 ms\n",
      "  Timesteps per second: 4.63\n",
      "  Nodes per timestep: 5740\n",
      "  Edges per timestep: 108495\n",
      "\n",
      "--- Batch of 8 (batch_size=8) ---\n",
      "Batch info: 45920 nodes, 869111 edges\n",
      "Running 5 warmup iterations...\n",
      "Running 50 timed iterations...\n",
      "  Completed 20/50 runs\n",
      "  Completed 40/50 runs\n",
      "  Total time: 1738.62 ms\n",
      "  Time per timestep: 217.33 ms\n",
      "  Timesteps per second: 4.60\n",
      "  Nodes per timestep: 5740\n",
      "  Edges per timestep: 108638\n",
      "\n",
      "3. REAL-TIME SIMULATION FEASIBILITY\n",
      "--------------------------------------------------\n",
      "============================================================\n",
      "REAL-TIME SIMULATION ANALYSIS\n",
      "============================================================\n",
      "Single timestep info: 5740 nodes, 108451 edges\n",
      "Batch size: 1\n",
      "Running 10 warmup iterations...\n",
      "Running 100 timed iterations...\n",
      "  Completed 20/100 runs\n",
      "  Completed 40/100 runs\n",
      "  Completed 60/100 runs\n",
      "  Completed 80/100 runs\n",
      "  Completed 100/100 runs\n",
      "\n",
      "Real-time Performance Analysis:\n",
      "  Single timestep time: 221.19 ms\n",
      "  Maximum FPS: 4.5\n",
      "  Target FPS: 60\n",
      "  âŒ Cannot achieve 60 FPS\n",
      "  ðŸ”§ Need 13.3x speedup\n",
      "\n",
      "Multi-step Prediction Times:\n",
      "   1 steps ahead:  221.2 ms (0.22 seconds)\n",
      "   5 steps ahead: 1105.9 ms (1.11 seconds)\n",
      "  10 steps ahead: 2211.9 ms (2.21 seconds)\n",
      "  20 steps ahead: 4423.7 ms (4.42 seconds)\n",
      "  50 steps ahead: 11059.3 ms (11.06 seconds)\n",
      "\n",
      "============================================================\n",
      "PERFORMANCE SUMMARY\n",
      "============================================================\n",
      "âœ¨ Single timestep inference: 225.5 ms\n",
      "ðŸš€ Maximum simulation speed: 4.4 FPS\n",
      "ðŸŽ¯ Real-time feasibility (60 FPS): âŒ NO\n",
      "ðŸ”§ Speedup needed: 13.3x\n",
      "ðŸ“Š Nodes per timestep: 5,740\n",
      "ðŸ“Š Edges per timestep: 108,451\n",
      "ðŸ’¾ Time per node: 39.283 Î¼s\n",
      "ðŸ’¾ Time per edge: 2.079 Î¼s\n"
     ]
    }
   ],
   "source": [
    "# Add this to your inference.ipynb\n",
    "def measure_single_timestep_inference(model, test_data_list, device, num_runs=100, warmup_runs=10):\n",
    "    \"\"\"\n",
    "    Measure inference time for a single physics timestep\n",
    "    \n",
    "    Args:\n",
    "        model: Trained GNN model\n",
    "        test_data_list: List of individual graph samples\n",
    "        device: Device to run on\n",
    "        num_runs: Number of inference runs for timing\n",
    "        warmup_runs: Number of warmup runs (excluded from timing)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with single-timestep timing metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create single-sample data loader (batch_size=1)\n",
    "    single_loader = DataLoader(test_data_list, batch_size=1, shuffle=False)\n",
    "    sample_batch = next(iter(single_loader))\n",
    "    sample_batch = sample_batch.to(device)\n",
    "    \n",
    "    print(f\"Single timestep info: {sample_batch.x.shape[0]} nodes, {sample_batch.edge_index.shape[1]} edges\")\n",
    "    print(f\"Batch size: {sample_batch.batch.max().item() + 1 if sample_batch.batch is not None else 1}\")\n",
    "    \n",
    "    # Warmup runs\n",
    "    print(f\"Running {warmup_runs} warmup iterations...\")\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup_runs):\n",
    "            _ = model(sample_batch.x, sample_batch.edge_index, sample_batch.batch)\n",
    "    \n",
    "    # Synchronize for accurate timing\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    elif device.type == 'mps':\n",
    "        torch.mps.synchronize()\n",
    "    \n",
    "    # Actual timing runs\n",
    "    print(f\"Running {num_runs} timed iterations...\")\n",
    "    inference_times = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_runs):\n",
    "            start_time = time.perf_counter()\n",
    "            \n",
    "            output = model(sample_batch.x, sample_batch.edge_index, sample_batch.batch)\n",
    "            \n",
    "            # Synchronize to ensure computation is complete\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            elif device.type == 'mps':\n",
    "                torch.mps.synchronize()\n",
    "            \n",
    "            end_time = time.perf_counter()\n",
    "            inference_times.append((end_time - start_time) * 1000)  # Convert to ms\n",
    "            \n",
    "            if (i + 1) % 20 == 0:\n",
    "                print(f\"  Completed {i + 1}/{num_runs} runs\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    inference_times = np.array(inference_times)\n",
    "    \n",
    "    metrics = {\n",
    "        'mean_time_ms': np.mean(inference_times),\n",
    "        'std_time_ms': np.std(inference_times),\n",
    "        'min_time_ms': np.min(inference_times),\n",
    "        'max_time_ms': np.max(inference_times),\n",
    "        'median_time_ms': np.median(inference_times),\n",
    "        'p95_time_ms': np.percentile(inference_times, 95),\n",
    "        'p99_time_ms': np.percentile(inference_times, 99),\n",
    "        'timesteps_per_sec': 1000 / np.mean(inference_times),\n",
    "        'num_nodes': sample_batch.x.shape[0],\n",
    "        'num_edges': sample_batch.edge_index.shape[1],\n",
    "        'time_per_node_us': (np.mean(inference_times) * 1000) / sample_batch.x.shape[0],\n",
    "        'time_per_edge_us': (np.mean(inference_times) * 1000) / sample_batch.edge_index.shape[1]\n",
    "    }\n",
    "    \n",
    "    return metrics, inference_times\n",
    "\n",
    "def compare_batch_vs_single_timestep(model, test_data_list, device):\n",
    "    \"\"\"\n",
    "    Compare batch processing vs single timestep processing\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"BATCH VS SINGLE TIMESTEP COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Test different configurations\n",
    "    configs = [\n",
    "        {\"batch_size\": 1, \"description\": \"Single Timestep\"},\n",
    "        {\"batch_size\": 2, \"description\": \"Batch of 2\"},\n",
    "        {\"batch_size\": 4, \"description\": \"Batch of 4\"},\n",
    "        {\"batch_size\": 8, \"description\": \"Batch of 8\"},\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for config in configs:\n",
    "        batch_size = config[\"batch_size\"]\n",
    "        description = config[\"description\"]\n",
    "        \n",
    "        print(f\"\\n--- {description} (batch_size={batch_size}) ---\")\n",
    "        \n",
    "        # Create data loader\n",
    "        loader = DataLoader(test_data_list[:batch_size*5], \n",
    "                          batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Measure timing\n",
    "        metrics, _ = measure_inference_time(model, loader, device, num_runs=50, warmup_runs=5)\n",
    "        \n",
    "        # Calculate per-timestep metrics\n",
    "        time_per_timestep = metrics['mean_time_ms'] / batch_size\n",
    "        timesteps_per_sec = batch_size * 1000 / metrics['mean_time_ms']\n",
    "        \n",
    "        result = {\n",
    "            'batch_size': batch_size,\n",
    "            'total_time_ms': metrics['mean_time_ms'],\n",
    "            'time_per_timestep_ms': time_per_timestep,\n",
    "            'timesteps_per_sec': timesteps_per_sec,\n",
    "            'nodes_per_timestep': metrics['num_nodes'] // batch_size,\n",
    "            'edges_per_timestep': metrics['num_edges'] // batch_size,\n",
    "        }\n",
    "        \n",
    "        results[batch_size] = result\n",
    "        \n",
    "        print(f\"  Total time: {result['total_time_ms']:.2f} ms\")\n",
    "        print(f\"  Time per timestep: {result['time_per_timestep_ms']:.2f} ms\")\n",
    "        print(f\"  Timesteps per second: {result['timesteps_per_sec']:.2f}\")\n",
    "        print(f\"  Nodes per timestep: {result['nodes_per_timestep']}\")\n",
    "        print(f\"  Edges per timestep: {result['edges_per_timestep']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def simulate_realtime_performance(model, test_data_list, device, target_fps=60):\n",
    "    \"\"\"\n",
    "    Simulate real-time performance requirements\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"REAL-TIME SIMULATION ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Measure single timestep performance\n",
    "    single_metrics, _ = measure_single_timestep_inference(\n",
    "        model, test_data_list, device, num_runs=100\n",
    "    )\n",
    "    \n",
    "    timestep_time_ms = single_metrics['mean_time_ms']\n",
    "    max_fps = 1000 / timestep_time_ms\n",
    "    \n",
    "    print(f\"\\nReal-time Performance Analysis:\")\n",
    "    print(f\"  Single timestep time: {timestep_time_ms:.2f} ms\")\n",
    "    print(f\"  Maximum FPS: {max_fps:.1f}\")\n",
    "    print(f\"  Target FPS: {target_fps}\")\n",
    "    \n",
    "    if max_fps >= target_fps:\n",
    "        print(f\"  âœ… Can achieve {target_fps} FPS (with {max_fps - target_fps:.1f} FPS headroom)\")\n",
    "        time_budget_ms = 1000 / target_fps\n",
    "        utilization = (timestep_time_ms / time_budget_ms) * 100\n",
    "        print(f\"  â±ï¸  GPU utilization: {utilization:.1f}%\")\n",
    "    else:\n",
    "        print(f\"  âŒ Cannot achieve {target_fps} FPS\")\n",
    "        print(f\"  ðŸ”§ Need {target_fps / max_fps:.1f}x speedup\")\n",
    "    \n",
    "    # Multi-step prediction analysis\n",
    "    steps_ahead = [1, 5, 10, 20, 50]\n",
    "    print(f\"\\nMulti-step Prediction Times:\")\n",
    "    for steps in steps_ahead:\n",
    "        total_time = timestep_time_ms * steps\n",
    "        print(f\"  {steps:2d} steps ahead: {total_time:6.1f} ms ({total_time/1000:.2f} seconds)\")\n",
    "    \n",
    "    return {\n",
    "        'timestep_time_ms': timestep_time_ms,\n",
    "        'max_fps': max_fps,\n",
    "        'can_achieve_target': max_fps >= target_fps,\n",
    "        'speedup_needed': target_fps / max_fps if max_fps < target_fps else 1.0\n",
    "    }\n",
    "\n",
    "# Run the single timestep benchmarks\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SINGLE TIMESTEP PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Pure single timestep measurement\n",
    "print(\"\\n1. MEASURING TRUE SINGLE TIMESTEP PERFORMANCE\")\n",
    "print(\"-\" * 50)\n",
    "single_metrics, single_times = measure_single_timestep_inference(\n",
    "    model, test_data_list, device, num_runs=100\n",
    ")\n",
    "\n",
    "print(f\"\\nSingle Timestep Statistics:\")\n",
    "print(f\"  Mean: {single_metrics['mean_time_ms']:.3f} ms\")\n",
    "print(f\"  Std:  {single_metrics['std_time_ms']:.3f} ms\")\n",
    "print(f\"  Min:  {single_metrics['min_time_ms']:.3f} ms\")\n",
    "print(f\"  Max:  {single_metrics['max_time_ms']:.3f} ms\")\n",
    "print(f\"  P95:  {single_metrics['p95_time_ms']:.3f} ms\")\n",
    "print(f\"  P99:  {single_metrics['p99_time_ms']:.3f} ms\")\n",
    "\n",
    "print(f\"\\nSingle Timestep Throughput:\")\n",
    "print(f\"  Timesteps/sec: {single_metrics['timesteps_per_sec']:.1f}\")\n",
    "print(f\"  Time per node: {single_metrics['time_per_node_us']:.3f} Î¼s\")\n",
    "print(f\"  Time per edge: {single_metrics['time_per_edge_us']:.3f} Î¼s\")\n",
    "\n",
    "# 2. Compare batch sizes\n",
    "print(\"\\n2. BATCH SIZE EFFICIENCY COMPARISON\")\n",
    "print(\"-\" * 50)\n",
    "batch_comparison = compare_batch_vs_single_timestep(model, test_data_list, device)\n",
    "\n",
    "# 3. Real-time simulation analysis\n",
    "print(\"\\n3. REAL-TIME SIMULATION FEASIBILITY\")\n",
    "print(\"-\" * 50)\n",
    "realtime_analysis = simulate_realtime_performance(model, test_data_list, device, target_fps=60)\n",
    "\n",
    "# 4. Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ¨ Single timestep inference: {single_metrics['mean_time_ms']:.1f} ms\")\n",
    "print(f\"ðŸš€ Maximum simulation speed: {single_metrics['timesteps_per_sec']:.1f} FPS\")\n",
    "print(f\"ðŸŽ¯ Real-time feasibility (60 FPS): {'âœ… YES' if realtime_analysis['can_achieve_target'] else 'âŒ NO'}\")\n",
    "\n",
    "if not realtime_analysis['can_achieve_target']:\n",
    "    print(f\"ðŸ”§ Speedup needed: {realtime_analysis['speedup_needed']:.1f}x\")\n",
    "\n",
    "print(f\"ðŸ“Š Nodes per timestep: {single_metrics['num_nodes']:,}\")\n",
    "print(f\"ðŸ“Š Edges per timestep: {single_metrics['num_edges']:,}\")\n",
    "print(f\"ðŸ’¾ Time per node: {single_metrics['time_per_node_us']:.3f} Î¼s\")\n",
    "print(f\"ðŸ’¾ Time per edge: {single_metrics['time_per_edge_us']:.3f} Î¼s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
